{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project3_updated.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pti6MR1lNEYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np\n",
        "import pickle #import dump\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from pickle import load\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.callbacks import EarlyStopping\n",
        "import sonnet_helper"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uqCFELYYm9o",
        "colab_type": "text"
      },
      "source": [
        "#Upload files for project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzL2t80my9lX",
        "colab_type": "code",
        "outputId": "dee99456-659f-4a60-c4f6-5b4c16d43a59",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "from google.colab import files\n",
        "Shakespeare = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1c0b09cf-1b46-4310-b625-14db360471ca\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-1c0b09cf-1b46-4310-b625-14db360471ca\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYIRqDBhYrwH",
        "colab_type": "code",
        "outputId": "590ad9c4-32f6-489a-e069-a7da3cabab24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "from sonnet_helper import sonnets_to_seqs\n",
        "from sonnet_helper import get_dictionary\n",
        "from sonnet_helper import load_sonnet_text\n",
        "def LSTM_load_sonnet_text(filename):\n",
        "    sonnets = []\n",
        "    sonnet_i = []\n",
        "    # import the raw data\n",
        "    data = open(filename,'r')\n",
        "    #lines = data.readlines()\n",
        "    #Nlines = len(lines)\n",
        "\n",
        "    # if the line starts with a number \n",
        "    while True:\n",
        "        line = data.readline()\n",
        "        if not line:\n",
        "            break\n",
        "        text = line.lower().split()\n",
        "        if len(text) > 0: # ignore empty lines\n",
        "            # if the line starts with a number, begin a new sonnet\n",
        "            if text[-1].isdigit():\n",
        "                sonnet_line = 0\n",
        "                if len(sonnet_i) > 0:\n",
        "                    sonnets.append(sonnet_i)\n",
        "                sonnet_i = []\n",
        "            # otherwise, append each word\n",
        "            else:\n",
        "                sonnet_i.append([])\n",
        "                for word in text:\n",
        "                    sonnet_i[sonnet_line].append(word)\n",
        "                sonnet_line += 1\n",
        "    sonnets.append(sonnet_i)\n",
        "    data.close()\n",
        "    return sonnets\n",
        "\n",
        "def LSTM_input_maker(Sequences, n_step, length = 40):\n",
        "  \"\"\"Takes sequences (poems) from the sonnets_to_seqs() function and makes an \n",
        "  array of 40 characters (starting at every n_step'th character) from each poem\n",
        "  \n",
        "  INPUTS:\n",
        "  Sequences: The output of the sonnets_to_seqs() fn. This will be a list of poems\n",
        "\n",
        "  length: The final length we want every output sequence to be. This is fixed to \n",
        "  40 as instructed #we should use another word than sequence everywhere its \n",
        "  getting confusing\n",
        "\n",
        "  n_step: The step length we take between the start of every sequence to make a \n",
        "  semi-redundant sequcence in our output\n",
        "\n",
        "  OUTPUT:\n",
        "  LSTM_seqs: a list of length N_poems, each containing the input vectors for \n",
        "  that poem \n",
        "  \"\"\"\n",
        "  N_poems = len(Sequences)#the number of poems\n",
        "  LSTM_seqs = [[] for _ in range(N_poems)] #our eventual output\n",
        "  length = length #to make the outputs the correct length\n",
        "\n",
        "  #loop through each of our poems\n",
        "  for i in range(N_poems):\n",
        "    #make each poem one continous string \n",
        "    poem = ' '.join(Sequences[i])\n",
        "\n",
        "    for j in range(length, len(poem)):\n",
        "      if j%n_step == 0:\n",
        "        #select sequence from each poem\n",
        "        x = poem[j-length:j]#+1]\n",
        "        # append each seuence into its correponding list in LSTM_seqs\n",
        "        LSTM_seqs[i].append(x) \n",
        "  return LSTM_seqs\n",
        "\n",
        "def LSTM_encode_seqs(LSTM_seqs_input):\n",
        "  N_poems = len(LSTM_seqs_input)\n",
        "  encoding_dictionary = get_dictionary(LSTM_seqs_input)\n",
        "  LSTM_encoded_seqs = [[] for _ in range(N_poems)]\n",
        "  for i in range(N_poems):\n",
        "    poem = LSTM_seqs_input[i]\n",
        "    for line in poem:\n",
        "      encoded_seq = [encoding_dictionary[char] for char in line]\n",
        "      LSTM_encoded_seqs[i].append(encoded_seq)\n",
        "\n",
        "  count = 0\n",
        "  for i in range(len(LSTM_encoded_seqs)):\n",
        "    poem = LSTM_encoded_seqs[i]\n",
        "    for j in range(len(poem)):\n",
        "      count += 1\n",
        "  #make this all into one array\n",
        "  count2 = 0\n",
        "  LSTM_encoded_array = np.zeros((count,len(LSTM_encoded_seqs[0][0])))\n",
        "  for i in range(len(LSTM_encoded_seqs)):\n",
        "    poem = LSTM_encoded_seqs[i]\n",
        "    for j in range(len(poem)):\n",
        "      LSTM_encoded_array[count2] = LSTM_encoded_seqs[i][j]\n",
        "      count2 +=1\n",
        "  return LSTM_encoded_array\n",
        "\n",
        "# generate a sequence of characters with a language model\n",
        "def sample(LSTM_y_hat, Temp=1.0):\n",
        "    \"\"\" Function that takes in predictions from LSTM, LSTM_y_hat, and smaples \n",
        "    them while considering the temperature parameter, Temp.\n",
        "\n",
        "    INPUTS:\n",
        "    LSTM_y_hat: The predictions output from the LSTM\n",
        "    Temp: The temperature parameter\n",
        "\n",
        "    OUTPUTS:\n",
        "    prediction: the smapled prediction from the probablitity distribution, \n",
        "    considering the temperature parameter\"\"\"\n",
        "    #First assure you have an array of the right shape\n",
        "    LSTM_y_hat = np.asarray(LSTM_y_hat).astype('float64')\n",
        "    #take the log of LSTM_y_hat to get log probablities and divide by Temp\n",
        "    log_yhat = np.log(LSTM_y_hat) / Temp\n",
        "    #calculate softmax considering temperature\n",
        "    num = np.exp(log_yhat)\n",
        "    den = np.sum(num)\n",
        "    softmax = num/den\n",
        "    #sample the softmax function\n",
        "    probs = np.random.multinomial(1, softmax, 1)\n",
        "    #makle a prediction\n",
        "    prediction = np.argmax(probs)\n",
        "    return prediction\n",
        "\n",
        "def generate(model, dictionary, input_text, output_length, temp):#remake\n",
        "    #save the right length of the encoded vectors for future refrence\n",
        "    encoded_len = len(input_text)\n",
        "    # Loop over every new character we want to make\n",
        "    for i in range(output_length):\n",
        "        # encode the characters as integers\n",
        "        encoded = []\n",
        "        encoded = [dictionary[char] for char in input_text]\n",
        "        #Change shape so this works as we continue to loop\n",
        "        encoded = pad_sequences([encoded], maxlen=encoded_len, truncating='pre')\n",
        "        #represent as a one hot vector and reshape to get in an appropriate shape\n",
        "        #for Keras to hangle\n",
        "        encoded = to_categorical(encoded, num_classes=len(dictionary))\n",
        "        encoded = encoded.reshape(1, encoded.shape[1], encoded.shape[2])\n",
        "        #predict the next character \n",
        "        z = model.predict(encoded, verbose = 0)[0]\n",
        "        #factor in temperature and sample the distribution\n",
        "        yhat = sample(z,temp)\n",
        "        #get letter of the sampled number from the dictionary\n",
        "        out_char = ''\n",
        "        for char, index in dictionary.items():\n",
        "            if index == yhat:\n",
        "                out_char = char\n",
        "                break\n",
        "        input_text += char\n",
        "    return input_text\n",
        "\n",
        "np.random.seed(0)\n",
        "sonnets = LSTM_load_sonnet_text('shakespeare.txt') #punctuation\n",
        "# sonnets = load_sonnet_text('shakespeare.txt') #no punctuation\n",
        "seqs = sonnets_to_seqs(sonnets)\n",
        "LSTM_seqs = LSTM_input_maker(seqs, 5, length = 40)\n",
        "dictionary = get_dictionary(LSTM_seqs)\n",
        "vocab_size = len(dictionary)\n",
        "LSTM_encoded_array = LSTM_encode_seqs(LSTM_seqs)\n",
        "\n",
        "print('sonnets = ', np.shape(sonnets))\n",
        "print('seqs = ', np.shape(seqs))\n",
        "print('LSTM_seqs = ', np.shape(LSTM_seqs))\n",
        "print('Vocab size = ', vocab_size)\n",
        "print('LSTM_encoded_array = ', np.shape(LSTM_encoded_array))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sonnets =  (154,)\n",
            "seqs =  (154,)\n",
            "LSTM_seqs =  (154,)\n",
            "Vocab size =  37\n",
            "LSTM_encoded_array =  (17533, 40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmF7ugoX0IW0",
        "colab_type": "text"
      },
      "source": [
        "# Train model on this encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISmn1nFq0LG5",
        "colab_type": "code",
        "outputId": "cb4647d1-2ba5-46a1-d99d-c5b69abc078d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        }
      },
      "source": [
        "# separate into input and output for first poem\n",
        "np.random.seed(0)\n",
        "X, y = LSTM_encoded_array[:,:-1], LSTM_encoded_array[:,-1]\n",
        "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
        "X = np.array(sequences)\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "\n",
        "#################### Model \n",
        "#outputs the probablity of being each word in the vocabulary\n",
        "model = Sequential()\n",
        "model.add(LSTM(150, input_shape=(X.shape[1], X.shape[2])))#150 hidden units\n",
        "model.add(Dense(vocab_size, activation='softmax'))#fully connected layer that \n",
        "####################\n",
        "\n",
        "\n",
        "#Define the model loss funtiomn and optimizer (CCE and Adam respectivley)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "#implament early stopping to assure convergence. Stop when accuracy stops increasing by \n",
        "#0.001 (min_delta).\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', min_delta=.001, patience= 10)\n",
        "#Fit the model\n",
        "model.fit(X, y, epochs = 1000,callbacks=[es], validation_split=0.2)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 14026 samples, validate on 3507 samples\n",
            "Epoch 1/1000\n",
            "14026/14026 [==============================] - 29s 2ms/step - loss: 2.8736 - acc: 0.2015 - val_loss: 2.6535 - val_acc: 0.2906\n",
            "Epoch 2/1000\n",
            "14026/14026 [==============================] - 27s 2ms/step - loss: 2.4593 - acc: 0.3087 - val_loss: 2.3333 - val_acc: 0.3370\n",
            "Epoch 3/1000\n",
            "14026/14026 [==============================] - 27s 2ms/step - loss: 2.2761 - acc: 0.3435 - val_loss: 2.2374 - val_acc: 0.3396\n",
            "Epoch 4/1000\n",
            "14026/14026 [==============================] - 27s 2ms/step - loss: 2.1790 - acc: 0.3649 - val_loss: 2.1770 - val_acc: 0.3707\n",
            "Epoch 5/1000\n",
            "14026/14026 [==============================] - 27s 2ms/step - loss: 2.1084 - acc: 0.3826 - val_loss: 2.1162 - val_acc: 0.3767\n",
            "Epoch 6/1000\n",
            "14026/14026 [==============================] - 26s 2ms/step - loss: 2.0450 - acc: 0.3957 - val_loss: 2.0919 - val_acc: 0.3878\n",
            "Epoch 7/1000\n",
            "14026/14026 [==============================] - 26s 2ms/step - loss: 1.9940 - acc: 0.4057 - val_loss: 2.0698 - val_acc: 0.3946\n",
            "Epoch 8/1000\n",
            "14026/14026 [==============================] - 26s 2ms/step - loss: 1.9468 - acc: 0.4184 - val_loss: 2.0190 - val_acc: 0.4129\n",
            "Epoch 9/1000\n",
            "14026/14026 [==============================] - 26s 2ms/step - loss: 1.9054 - acc: 0.4311 - val_loss: 2.0098 - val_acc: 0.4052\n",
            "Epoch 10/1000\n",
            "14026/14026 [==============================] - 27s 2ms/step - loss: 1.8648 - acc: 0.4368 - val_loss: 1.9780 - val_acc: 0.4132\n",
            "Epoch 11/1000\n",
            "14026/14026 [==============================] - 27s 2ms/step - loss: 1.8243 - acc: 0.4558 - val_loss: 1.9625 - val_acc: 0.4169\n",
            "Epoch 12/1000\n",
            "14026/14026 [==============================] - 27s 2ms/step - loss: 1.7849 - acc: 0.4599 - val_loss: 1.9592 - val_acc: 0.4157\n",
            "Epoch 13/1000\n",
            "14026/14026 [==============================] - 27s 2ms/step - loss: 1.7488 - acc: 0.4703 - val_loss: 1.9546 - val_acc: 0.4172\n",
            "Epoch 14/1000\n",
            "14026/14026 [==============================] - 27s 2ms/step - loss: 1.7102 - acc: 0.4804 - val_loss: 1.9402 - val_acc: 0.4283\n",
            "Epoch 15/1000\n",
            "14026/14026 [==============================] - 27s 2ms/step - loss: 1.6711 - acc: 0.4939 - val_loss: 1.9527 - val_acc: 0.4249\n",
            "Epoch 16/1000\n",
            "14026/14026 [==============================] - 27s 2ms/step - loss: 1.6312 - acc: 0.4999 - val_loss: 1.9524 - val_acc: 0.4254\n",
            "Epoch 17/1000\n",
            "14026/14026 [==============================] - 27s 2ms/step - loss: 1.5901 - acc: 0.5120 - val_loss: 1.9636 - val_acc: 0.4229\n",
            "Epoch 18/1000\n",
            "14026/14026 [==============================] - 27s 2ms/step - loss: 1.5464 - acc: 0.5238 - val_loss: 1.9719 - val_acc: 0.4212\n",
            "Epoch 19/1000\n",
            "14026/14026 [==============================] - 27s 2ms/step - loss: 1.4997 - acc: 0.5418 - val_loss: 1.9903 - val_acc: 0.4220\n",
            "Epoch 20/1000\n",
            "14026/14026 [==============================] - 27s 2ms/step - loss: 1.4486 - acc: 0.5523 - val_loss: 2.0062 - val_acc: 0.4189\n",
            "Epoch 21/1000\n",
            "14026/14026 [==============================] - 27s 2ms/step - loss: 1.3970 - acc: 0.5730 - val_loss: 2.0246 - val_acc: 0.4120\n",
            "Epoch 22/1000\n",
            "14026/14026 [==============================] - 27s 2ms/step - loss: 1.3422 - acc: 0.5888 - val_loss: 2.0657 - val_acc: 0.4112\n",
            "Epoch 23/1000\n",
            "14026/14026 [==============================] - 27s 2ms/step - loss: 1.2849 - acc: 0.6087 - val_loss: 2.0883 - val_acc: 0.4032\n",
            "Epoch 24/1000\n",
            "14026/14026 [==============================] - 27s 2ms/step - loss: 1.2270 - acc: 0.6269 - val_loss: 2.1303 - val_acc: 0.4038\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f352d764198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0seYf-9rBWlt",
        "colab_type": "text"
      },
      "source": [
        "## Temperature modficiations: With punctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0flAnnUJncPJ",
        "colab_type": "code",
        "outputId": "b937a96a-a496-49ee-9e23-7b5149d9ac15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#temp = 1.5\n",
        "np.random.seed(0)\n",
        "test = 'shall i compare thee to a summer\\'s day?' \n",
        "print(generate(model, dictionary, test, 1000, 1.5))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shall i compare thee to a summer's day?: themt my thight, where brndsat low iin his-sink. lon stould usroc an by thinh cons, wobch thio merrey, ele, niret decvivuds'st whey zoat you sne, yot, repult ed, agreefef the i gans haw, to cy remir) of formandas that my uryes ing.ownith whe warlthi gorn to see! (dexingle thougaray shouk, faolthly sight!''t qleclev's dyel, whih coundye ut thy, ere reatefi, hin, and thou aze wfemiel, by this be.'st my shispsn, burss ht as thous flost. hoc foors hif, nitsilced! ame host awe, for have corragl.s pirspiet, weike :ut doth canet ushre, chee! bue iacome's le.se, in which is shouguty sweease ow my hend.'to elt that is shink om sis cwar) farires heprasy merustif wolle't otreast, is pabunce his riallcaycemek, and migitsg ank veronaded's praverhs oin their foadl (lorfuts acy in the, aflonk yon, eddqonforgh fad tfemen that for thou thinitius loovrics hammakiby wathe pordlcelen, sunkno padse, thy your anes his, i barruthouch my arkn. leastod, ni stalf ye'll keods, conleed scepsorr my meir, love, t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPNvsfiCBzqd",
        "colab_type": "code",
        "outputId": "5ac3fd08-5c69-4486-8931-64285c86622b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#temp = 0.75\n",
        "np.random.seed(0)\n",
        "test = 'shall i compare thee to a summer\\'s day?' \n",
        "print(generate(model, dictionary, test, 1000, 0.75))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shall i compare thee to a summer's day? love that rith so so, love the tome all stime, that roppsede, thou shalk of plucter in gith faints thes wout o' his, and thingand cain! afthrrted singh burece aconearing; aylling me all ir mime, but thing rage the meat bast mad'st by with cheir the torn formy the were strind, our shece thou my ingred speft, to that that whowglidur will, a be thing ey seaved, i me thee de tome preart of net my mised, stour aitedy dosprowing deare a ouking doow thy houb to glass will mare make my have thee het besty if lopburt by the some to be. poor well deam be thou sake, bur fird wall have mome hom then, for thee art proef, and that whowk it mose? not hat be mise, mud ast bloud, no sermest do bronguth unmes in the formely dith love a thou are me welt bo die! be, that kish in my sape not for thee week, that corsed with whis bettal that so leas acay and asl in thy hour thy shor that steet, or sthat be thou shoundst apce so levir, my lend-st low in yer wrety for thay stirve the live, thy willo st me, al\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttByRuSsB9LV",
        "colab_type": "code",
        "outputId": "dad87dcd-1d42-46ca-80be-d830923216a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#temp = 0.25\n",
        "np.random.seed(0)\n",
        "test = 'shall i compare thee to a summer\\'s day?' \n",
        "print(generate(model, dictionary, test, 1000, 0.25))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shall i compare thee to a summer's day? in the sanke that thou shounds speaty, for that mout my me, thou my soor me betore, that beat of thee with thou hart thou stare thou beauty the world wilt it me thou steat, thou mast so love, that thou show thou steaty love, that thou mast make my love, that thou makes but whet to thou shomp, to beauty love and there ble save but that is my so for have that heart that which i be thou makes the gave the world all the werot my lead, and love at mour stall the fair that for thee well that whot so love, then beart that is my soof love the past, and the ear have love a dour to my in to thy should in mest of forst of me the praces of thee me thou shall that have to me thy sullle my shall here, that though that moth do thy should and ast, and to the stall that so fors of the praces of thee shen thou sweet thou steatt, to gract in then a aintall my lear, and the gracte thee me thou sund, and that that thou shall that hast love, the ear hast of thee hish whit thou stall thou strong that my lea\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIfrloVP3xfh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pickle.dump(model, open( \"Model1_nstep5_.p\", \"wb\" ) )\n",
        "# pickle.dump(model, open( \"Model1_nstep5_val.p\", \"wb\" ) )\n",
        "# pickle.dump(model, open( \"Model1_nstep5_val_max.p\", \"wb\" ) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAQPsWVoNO78",
        "colab_type": "text"
      },
      "source": [
        "## Gasen shat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBZnNQIiNifa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pickle_in = open(\"Model1_nstep5_val_max.p\",\"rb\")\n",
        "# gasen_shat =  pickle.load(pickle_in)\n",
        "# test = 'shall i compare thee to a summer\\'s day ' \n",
        "# print(generate(gasen_shat, dictionary, 39, test, 200, .01))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeGKo3w7icms",
        "colab_type": "text"
      },
      "source": [
        "Temperature mods: no punctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InunhMCTW_b4",
        "colab_type": "code",
        "outputId": "03f02573-9de4-4c0f-d0fe-99abf63e5bb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# #temp = 1.5\n",
        "# print(generate(gasen_shat, dictionary, 39, test, 800, 1.5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shall i compare thee to a summer's day you matr faroty 's vert or sort 's my mored breadth nor atrein tith weels to th's deed-ielencereief worle ubenins opd sips no spo-gult) name fres) sipe must thou lors with thou sage stimpst of fliceds no menceect vist a com (norrovers speedboty did mish ever atoin not sise comoll that do in if gat anogh maghe maseakes alled''s liveled me ie not for how nof gindy sastiant of noke nge all frimm rosting thus swowlusp listingt wet sunase's when thee afof thy saroos butring corno's what i haben by of eflef eyesvreslled deaines which lom leave uree berled of wreess brot ill hom not fore canctoin bey lives bast)clle se maser thy covbost i dormintoond shene oos jorgenan whi(e aud hakr farot belury od kikioss pyeceiss floe that paistika gosw sece coseares worth nep pete till wruce shouls me fuce yo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-17_iqg0nGhV",
        "colab_type": "code",
        "outputId": "ba427595-27e1-4944-e45d-3503a302b399",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# #temp = 0.75\n",
        "# print(generate(gasen_shat, dictionary, 39, test, 800, 0.75))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shall i compare thee to a summer's day at in whiteling by panseccare that do all the vertae sweet leaken my sode sheis sest to mase you to the flowe'se mases foor llove the condore s mase should theur hough thee drees to the every or mory gorr where the ropbel stlond our that do hord but thou wartunl fime my hought in peased when i have and to their sthencess ars all the prous sweet nous morn andst excull by for ow ald love the cond wond his steis to time and hough) not fave of mall fremmmy trowh hears ou truts to sugivest i cond and for you still thy so bore nom made the verower had you such morth ne will tas still the decold can you is ave the flooker's rastath me to coll spaintsto no me since to fard with thie the can you is joth my live he cor of and there are farres of the contore sting true all ay the wind ou prutse when \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQq2B1afnV2Q",
        "colab_type": "code",
        "outputId": "9005905b-5e60-4cc6-9164-e32bb439b396",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# #temp = 0.25\n",
        "# print(generate(gasen_shat, dictionary, 39, test, 800, 0.75))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shall i compare thee to a summer's day to my sweet-least bntrenging with heaveng fo montee shene of fromm nex the farower hange arth if the fared plove the brow suce shis wort guad whine east thy have's spone sinf ot th tet his ss to tut ou wolt that kish morth and not pariss tormerd cortengt troull spin tori king to thy every as will bust in with the worth is weld but mancer's con bor dusl mind a chell of with of hours mpreast inco beate thee kill time now selw suchot love urse not feam such most when i cave the beathe of for stimest bose net trees in so bore when whece hou hot ded chot buck of me my shelld drinklest thou art gove srow and should do the mind of suce so love that kell to mast whor urouble no farou all vindded arse misteare wish the ill age) not least wrich in thy parse thou brow ands one sweat for thy bact dece\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTwHuz4u0IYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}